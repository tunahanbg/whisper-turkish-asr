# ASR System - Architecture & Module Flow

## üìê System Architecture Overview

This document provides a deep technical overview of how the ASR system works, module dependencies, and execution flow.

---

## üéØ Core Design Principles

1. **Config-Driven**: All parameters centralized in `config/config.yaml`
2. **Modular**: Each module has single responsibility
3. **Factory Pattern**: Easy model switching via ModelManager
4. **Singleton Pattern**: Single Config instance across application
5. **DRY**: No code duplication, reusable utilities

---

## üîÑ Execution Flow

### 1. Application Startup (Streamlit UI)

```
app.py (entry point)
  ‚îî‚îÄ> src/ui/app.py::main()
      ‚îú‚îÄ> initialize_session_state()
      ‚îÇ   ‚îú‚îÄ> ModelManager (lazy, not loaded yet)
      ‚îÇ   ‚îú‚îÄ> AudioPreprocessor
      ‚îÇ   ‚îú‚îÄ> VoiceActivityDetector
      ‚îÇ   ‚îî‚îÄ> AudioFileHandler
      ‚îî‚îÄ> sidebar_settings() + main tabs
```

### 2. Model Loading Flow

```
User clicks "Load Model" or starts transcription
  ‚îî‚îÄ> load_model() in ui/app.py
      ‚îî‚îÄ> ModelManager.load_model()
          ‚îú‚îÄ> Read config (model.name, model.variant, model.device)
          ‚îú‚îÄ> Factory pattern selects implementation:
          ‚îÇ   ‚îú‚îÄ> faster-whisper ‚Üí FasterWhisperASR
          ‚îÇ   ‚îî‚îÄ> whisper ‚Üí WhisperASR
          ‚îî‚îÄ> model.load()
              ‚îú‚îÄ> FasterWhisperASR: Load CTranslate2 model
              ‚îî‚îÄ> WhisperASR: Load OpenAI Whisper or HF Transformers (quantized)
```

**Model Types:**
- **Faster-Whisper** (Primary): CTranslate2 optimized, 3-4x faster, int8 quantization
- **Whisper (Standard)**: OpenAI's original implementation
- **Whisper (Quantized)**: HuggingFace Transformers with INT4 quantization

### 3. File Upload Transcription Flow

```
User uploads audio file
  ‚îî‚îÄ> file_upload_tab()
      ‚îî‚îÄ> AudioFileHandler.load(file_path)
          ‚îú‚îÄ> librosa.load() ‚Üí numpy array (mono, 16kHz)
          ‚îî‚îÄ> validate_audio()
      ‚îî‚îÄ> transcribe_audio(audio, language)
          ‚îú‚îÄ> load_model() (if not loaded)
          ‚îú‚îÄ> Normalize audio ([-1, 1] range)
          ‚îú‚îÄ> Check silence (RMS < 0.01 ‚Üí reject)
          ‚îî‚îÄ> model.transcribe(audio, language)
              ‚îú‚îÄ> FasterWhisperASR.transcribe()
              ‚îÇ   ‚îú‚îÄ> WhisperModel.transcribe() (CTranslate2)
              ‚îÇ   ‚îî‚îÄ> Return segments + text
              ‚îî‚îÄ> WhisperASR.transcribe()
                  ‚îú‚îÄ> Standard: whisper.transcribe()
                  ‚îî‚îÄ> Quantized: HF generate() + decode()
          ‚îî‚îÄ> format_output() ‚Üí display_transcription_result()
```

### 4. Microphone Recording Flow

```
User clicks "Start Recording"
  ‚îî‚îÄ> microphone_tab()
      ‚îî‚îÄ> AudioRecorder(vad, input_gain)
          ‚îî‚îÄ> start_recording()
              ‚îú‚îÄ> sounddevice.InputStream (float32, 16kHz, mono)
              ‚îú‚îÄ> Background thread: _process_audio()
              ‚îÇ   ‚îú‚îÄ> Collect chunks in queue
              ‚îÇ   ‚îú‚îÄ> Apply input_gain (amplify signal)
              ‚îÇ   ‚îî‚îÄ> VAD check: should_stop_recording()
              ‚îÇ       ‚îî‚îÄ> VoiceActivityDetector.should_stop_recording()
              ‚îÇ           ‚îú‚îÄ> Silero VAD model
              ‚îÇ           ‚îú‚îÄ> detect_speech() ‚Üí segments
              ‚îÇ           ‚îî‚îÄ> check_silence_duration()
              ‚îî‚îÄ> Auto-stop when silence > 10s

User clicks "Stop Recording" (or VAD auto-stops)
  ‚îî‚îÄ> stop_recording()
      ‚îú‚îÄ> Close stream
      ‚îú‚îÄ> Concatenate chunks ‚Üí numpy array
      ‚îî‚îÄ> transcribe_audio(audio, language)
          ‚îî‚îÄ> [Same as file upload flow]
```

---

## üì¶ Module Dependencies

### Core Modules

#### 1. Config System (`config/`)
```
config/__init__.py (Config class - Singleton)
  ‚îî‚îÄ> config.yaml (YAML file)
      ‚îú‚îÄ> model: name, variant, device, compute_type
      ‚îú‚îÄ> audio: sample_rate, preprocessing settings
      ‚îú‚îÄ> vad: threshold, silence duration
      ‚îú‚îÄ> transcription: beam_size, temperature, etc.
      ‚îî‚îÄ> evaluation: test settings
```

**Key Methods:**
- `config.get('model.variant')` - Nested key access
- `config.set('model.variant', 'small')` - Runtime updates
- `config.model_config` - Property shortcuts

#### 2. Model Management (`src/models/`)

**Hierarchy:**
```
BaseASR (abstract)
  ‚îú‚îÄ> WhisperASR (Standard + Quantized HF)
  ‚îî‚îÄ> FasterWhisperASR (CTranslate2)

ModelManager (Factory)
  ‚îî‚îÄ> Selects implementation based on config
```

**FasterWhisperASR** (Primary):
- Uses CTranslate2 backend
- 3-4x faster than standard Whisper
- int8 quantization by default
- Supports custom/fine-tuned models
- Device: CPU, CUDA (no MPS support)

**WhisperASR**:
- Standard: OpenAI whisper library
- Quantized: HuggingFace Transformers
  - INT4 quantization (compressed-tensors)
  - Large-v3 model: WER 19% (best accuracy)
  - Slower but more accurate

#### 3. Audio Processing (`src/audio/`)

**AudioFileHandler**:
- Load: librosa.load() ‚Üí numpy array
- Save: soundfile.write()
- Validate: duration, silence check
- Supports: FLAC, WAV, MP3, M4A, OGG

**AudioRecorder**:
- Real-time microphone capture
- sounddevice.InputStream (float32)
- Background thread for processing
- VAD integration for auto-stop
- Input gain control (amplify weak signals)

#### 4. Preprocessing (`src/preprocessing/`)

**AudioPreprocessor**:
- **Normalize**: Peak normalization (max = 1.0)
- **Trim Silence**: librosa.effects.trim()
- **Denoise**: noisereduce (spectral gating)
- **Resample**: librosa.resample()

**Note**: Preprocessing is **DISABLED** by default in config because Whisper has built-in preprocessing.

**VoiceActivityDetector**:
- Silero VAD model (torch.hub)
- Detect speech segments
- Calculate silence duration
- Auto-stop recording trigger

#### 5. Utilities (`src/utils/`)

**audio_utils.py**:
- `load_audio()` - Load with librosa
- `save_audio()` - Save with soundfile
- `validate_audio()` - Duration + silence check
- `format_timestamp()` - HH:MM:SS.mmm format

**logger_setup.py**:
- Loguru configuration
- File + console logging
- Rotation: 100 MB
- Retention: 1 week

---

## üß™ Test & Evaluation System

### Test Framework (`tests/evaluation/`)

**ASRBenchmarker** (Main orchestrator):
- Load test set (ground_truth.json)
- Run single test with model config
- Model comparison (tiny, base, small, medium, large-v3-w4a16)
- Implementation comparison (whisper vs faster-whisper)
- Preprocessing comparison

**Metrics** (`metrics.py`):
- **WER** (Word Error Rate): jiwer library
- **CER** (Character Error Rate): Character-level accuracy
- **RTF** (Real-Time Factor): processing_time / audio_duration
- **Normalized metrics**: Turkish text normalization (case, punctuation)

**ResourceMonitor** (`resource_monitor.py`):
- Context manager for resource tracking
- CPU usage: psutil.cpu_percent()
- Memory: psutil.Process().memory_info()
- Peak memory tracking

**ReportGenerator** (`report_generator.py`):
- Export formats: JSON, CSV, Markdown, LaTeX
- Comparison tables
- Summary statistics

### Test Scripts (`tests/scripts/`)

**run_benchmarks.py** (Main runner):
- Modes: full, models, implementations, preprocessing, quick
- Sample limiting
- Multi-format export

**compare_models.py**:
- Quick model comparison
- 50-150 samples
- Table output

**quick_test.py**:
- 5 samples
- Fast validation
- System check

**prepare_test_set.py**:
- Random sample selection from data/raw/TR/
- Generate ground_truth.json
- 300 samples default

---

## üîç Data Flow Example: File Upload Transcription

```
1. User uploads "audio.mp3" (3 minutes, Turkish)

2. AudioFileHandler.load()
   Input: "audio.mp3"
   Output: numpy array (2,880,000 samples @ 16kHz), sr=16000

3. Validation
   - Duration: 180s ‚úì
   - Not silent: max amplitude > 0.02 ‚úì

4. Model Selection (from config)
   - model.name: "faster-whisper"
   - model.variant: "medium"
   - model.compute_type: "int8"

5. ModelManager.load_model()
   - Factory selects: FasterWhisperASR
   - Load CTranslate2 model: "medium"
   - Device: CPU, compute_type: int8

6. Transcription
   FasterWhisperASR.transcribe(audio, language="tr")
   - Input: numpy array (float32, mono, 16kHz)
   - CTranslate2 inference
   - Beam search (beam_size=5)
   - Generate segments with timestamps

7. Output Formatting
   {
     'text': "Merhaba, bu bir test kaydƒ±dƒ±r...",
     'language': "tr",
     'segments': [
       {'start': 0.0, 'end': 2.5, 'text': "Merhaba,"},
       {'start': 2.5, 'end': 5.0, 'text': "bu bir test kaydƒ±dƒ±r..."}
     ],
     'processing_time': 45.2,
     'audio_duration': 180.0,
     'rtf': 0.25
   }

8. UI Display
   - Text in expandable card
   - Segments with timestamps
   - Metrics: WER, RTF, duration
   - Download button
```

---

## üéõÔ∏è Configuration System

### Config Hierarchy

```yaml
config.yaml
‚îú‚îÄ> model:
‚îÇ   ‚îú‚îÄ> name: "faster-whisper" | "whisper"
‚îÇ   ‚îú‚îÄ> variant: "tiny" | "base" | "small" | "medium" | "large-v3-w4a16"
‚îÇ   ‚îú‚îÄ> device: "cpu" | "cuda" | "mps"
‚îÇ   ‚îú‚îÄ> compute_type: "int8" | "float16" | "float32"
‚îÇ   ‚îî‚îÄ> model_path: null | "./path/to/custom/model"
‚îÇ
‚îú‚îÄ> audio:
‚îÇ   ‚îú‚îÄ> sample_rate: 16000
‚îÇ   ‚îú‚îÄ> channels: 1
‚îÇ   ‚îî‚îÄ> preprocessing:
‚îÇ       ‚îú‚îÄ> enabled: false  # Disabled by default
‚îÇ       ‚îú‚îÄ> normalize: false
‚îÇ       ‚îú‚îÄ> trim_silence: false
‚îÇ       ‚îî‚îÄ> denoise: false
‚îÇ
‚îú‚îÄ> vad:
‚îÇ   ‚îú‚îÄ> enabled: true
‚îÇ   ‚îú‚îÄ> threshold: 0.5
‚îÇ   ‚îú‚îÄ> min_silence_duration_ms: 10000  # 10 seconds
‚îÇ   ‚îî‚îÄ> model: "silero"
‚îÇ
‚îú‚îÄ> transcription:
‚îÇ   ‚îú‚îÄ> task: "transcribe"
‚îÇ   ‚îú‚îÄ> temperature: 0.0
‚îÇ   ‚îú‚îÄ> beam_size: 5
‚îÇ   ‚îî‚îÄ> word_timestamps: false
‚îÇ
‚îî‚îÄ> evaluation:
    ‚îú‚îÄ> test_set_path: "./tests/data/test_set"
    ‚îú‚îÄ> model_variants: ["tiny", "base", "small", "medium", "large-v3-w4a16"]
    ‚îî‚îÄ> custom_models:
        ‚îî‚îÄ> large_v3_quantized:
            ‚îú‚îÄ> name: "whisper"
            ‚îú‚îÄ> variant: "large-v3-w4a16"
            ‚îú‚îÄ> model_path: "./checkpoints/hf_models/whisper-large-v3-w4a16"
            ‚îî‚îÄ> device: "cpu"
```

### Runtime Config Updates

```python
from config import config

# Read
variant = config.get('model.variant')  # "base"

# Update
config.set('model.variant', 'small')

# Save to file
config.save()
```

---

## üöÄ Performance Characteristics

### Model Comparison (300 samples, MacBook M4 Pro)

| Model | Implementation | WER | RTF | Memory | Notes |
|-------|---------------|-----|-----|--------|-------|
| Tiny | faster-whisper | 71% | 0.09x | 0.87 GB | Fastest, least accurate |
| Base | faster-whisper | 53% | 0.13x | 0.84 GB | Good balance |
| Small | faster-whisper | 36% | 0.22x | 0.85 GB | Better accuracy |
| Medium | faster-whisper | 27% | 0.39x | 0.86 GB | Recommended |
| Large-v3 INT4 | whisper (quantized) | **19%** | 33.7x | 2.1 GB | Best accuracy, very slow |

**RTF (Real-Time Factor)**:
- 0.1x = 10x faster than real-time
- 1.0x = real-time
- 33.7x = 33.7x slower than real-time

---

## üîê Security & Privacy

- **100% Local**: No cloud API calls
- **No Data Collection**: All processing on-device
- **No Network Required**: Models downloaded once, cached locally
- **Open Source**: Full code transparency

---

## üìö Key Technologies

| Component | Technology | Purpose |
|-----------|-----------|---------|
| ASR Engine | Faster-Whisper (CTranslate2) | Fast inference |
| Quantization | compressed-tensors (INT4) | Model compression |
| VAD | Silero VAD | Speech detection |
| Audio I/O | sounddevice, librosa, soundfile | Recording & file handling |
| UI | Streamlit | Web interface |
| Testing | pytest, jiwer | Unit tests & WER/CER |
| Monitoring | psutil | Resource tracking |
| Config | PyYAML | Configuration management |
| Logging | loguru | Structured logging |

---

## üéì For Developers

### Adding a New Model

1. Create new class inheriting from `BaseASR`
2. Implement: `load()`, `transcribe()`, `unload()`
3. Add to `ModelManager._load_*()` factory method
4. Update `ModelType` enum
5. Add config entry in `config.yaml`

### Adding a New Metric

1. Add function to `tests/evaluation/metrics.py`
2. Update `ASRBenchmarker.run_single_test()` to calculate it
3. Add to report templates in `ReportGenerator`

### Debugging Tips

- Check logs: `logs/asr_system.log`
- Enable debug logging: `config.set('logging.level', 'DEBUG')`
- Use `quick_test.py` for fast iteration
- Monitor resources with `ResourceMonitor`

---

## üìä Project Statistics

- **Total Modules**: 20+ Python modules
- **Lines of Code**: ~5,000 (excluding tests)
- **Test Coverage**: Unit tests + integration benchmarks
- **Supported Languages**: Turkish, English (extensible)
- **Model Variants**: 5 (tiny, base, small, medium, large-v3-w4a16)
- **Test Dataset**: 300 samples (Mozilla Common Voice Turkish)

---

**Last Updated**: December 2025  
**Project**: Gazi University Computer Engineering - ASR System

